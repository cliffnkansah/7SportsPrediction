# -*- coding: utf-8 -*-
"""7SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v69BDk1vopsDh3LwNEInLrGJaGMakClN
"""

#import all required packages
import pandas as pd
import numpy as np
import os
import sklearn
import xgboost as xgb
import matplotlib.pyplot as plt
import joblib
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error , mean_squared_error, make_scorer
from sklearn import tree, metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV,KFold
from sklearn.preprocessing import LabelEncoder

#mounting our google drive
from google.colab import drive
drive.mount('/content/drive')

"""**LOADING THE FIFA 22 DATASET**

"""

##importing our dataset
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AI/Midsem/players_21.csv')

"""## **Demonstrate the data preparation & feature extraction process [5]**"""

df.head()

df.info()

"""HANDLING THE NULL NUMERICAL VALUES"""

#checking for missing values
numeric_columns = df.select_dtypes(include=['float64','int64'])
missing_values = numeric_columns.isnull().sum()
missing_values

#replacing missing values with mean of its columns
df[numeric_columns.columns] = df[numeric_columns.columns].fillna(numeric_columns.mean())
df

#columns of type object
objects_columns = df.select_dtypes(include=['object'])
objects_columns

# dropping the follwing columns from the dataset for encoding
positions =  df[['ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram',
                  'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb',
                  'lb', 'lcb', 'cb', 'rcb', 'rb', 'gk']]

df = df.drop(columns=positions)

#encoding the categorical variables using labelecoding
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

# Iterate over each column in the DataFrame and apply label encoding
positions_copy = positions.copy()

# Iterate over each column in the copy and apply label encoding
for column in positions_copy.columns:
    positions_copy[column] = label_encoder.fit_transform(positions_copy[column])

positions

#adding the encoded values to the dataset
df = pd.concat([df, positions_copy], axis=1)
df

objects_columns = df.select_dtypes(include=['object'])
# dropping all unwanted categorica variables
# we dropped all other columns aside from the positions because from the description the player any of the urls are not necessary
# When predicting a player's overall rating, not all columns in your dataset may be directly relevant to the target variable (overall rating)
#Some features, like a player's name, date of birth, or unique identifier,league level may not have a significant impact on the rating prediction.
# We Removed such columns can help reduce noise in the data and improve the accuracy of the prediction model.
# This allows the model to focus on the most informative features that contribute to a player's overall rating which after further research were the numerical variables
#and the categorical variables which talks about the player positions
df = df.select_dtypes(exclude=['object'])
df

for column in df.columns:
    print(column)

"""### **Create feature subsets that show maximum correlation with the dependent variable**"""

#featured subsets using correlation
feature_corr = df.corr()
feature_corr = feature_corr['overall'].sort_values(ascending=False)
print(feature_corr)

#sorting the feature correlation in descending order
feature_corr= feature_corr.sort_values(ascending=False)
feature_corr

##  features with the highest absolute correlations from 0.5 and above to see which ones have great correlation with overall
highest_correlations = feature_corr[1:35]
highest_correlations

#Correlation does not imply causation
#Just because two variables are highly correlated doesn't mean that one directly causes the other
#It's possible that they are both influenced by a third variable, or the relationship is coincidental
#therefore after more testing and training we came up with these features for training
#features to be used for training
#features to be used for training
subsetx = [
    'movement_reactions',
    'skill_dribbling',
    'passing',
    'potential',
    'dribbling',
    'attacking_short_passing',
    'physic',
    'skill_long_passing',
    'movement_agility',
    'skill_moves',
    'shooting',
    'skill_ball_control',
    'mentality_vision',
    'weight_kg',
    'attacking_crossing'
]
feature_subsetx =df[subsetx]

# scaling our independent variables
scx = StandardScaler()
scaledx = scx.fit_transform(feature_subsetx)
feature_subsetx = pd.DataFrame(scaledx,columns = feature_subsetx.columns)
feature_subsetx

# pickling the model #creating the scaler model
import pickle
pickle_out = open("scaler.pkl", "wb")
pickle.dump(scx, pickle_out)
pickle_out.close()

"""### **Create and train a suitable machine learning model with cross-validation that can predict a player's rating. [5]**"""

#defining our X and Y for training
Y = df['overall']
X = feature_subsetx

Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y,test_size=0.2,random_state=42)
model_rms_scores=[]

"""**DecisionTreeRegressor**

### **Measure the model's performance and fine-tune it as a process of optimization. [5]**
"""

regressor = DecisionTreeRegressor()
regressor.fit(Xtrain,Ytrain)
y_pred = regressor.predict(Xtest)
mae=mean_absolute_error(y_pred,Ytest)
print("Mean Absolute Error:", mae)

mse= mean_squared_error(y_pred,Ytest)
print("Mean Squared Error:", mse)

rmse1 = np.sqrt(mse)
model_rms_scores.append(rmse1)
print("Root Mean Squared Error:",rmse1)

"""**XGBOOST**"""

xgbr = XGBRegressor(
  objective = "reg:squarederror",
  n_estimators =200,
  learning_rate = 0.1,
  max_depth = 50
)

xgbr.fit(Xtrain,Ytrain)
y_pred = xgbr.predict(Xtest)

mae=mean_absolute_error(y_pred,Ytest)
print("Mean Absolute Error:", mae)

mse= mean_squared_error(y_pred,Ytest)
print("Mean Squared Error:", mse)

rmse2 = np.sqrt(mse)
model_rms_scores.append(rmse2)
print("Root Mean Squared Error:",rmse2)

"""**GRADIENT** **BOOST**"""

gb_regressor = GradientBoostingRegressor(
  loss = "squared_error",
  n_estimators =200,
  learning_rate = 0.1,
  max_depth = 50
)

gb_regressor.fit(Xtrain,Ytrain)
y_pred = gb_regressor.predict(Xtest)

mae=mean_absolute_error(y_pred,Ytest)
print("Mean Absolute Error:", mae)

mse= mean_squared_error(y_pred,Ytest)
print("Mean Squared Error:", mse)

rmse3 = np.sqrt(mse)
model_rms_scores.append(rmse3)
print("Root Mean Squared Error:",rmse3)

"""**VOTING REGRESSOR**"""

linear_regressor_model = LinearRegression()
xgboost_regressor = XGBRegressor()
random_forest_regressor = RandomForestRegressor()
gradient_boosting_regressor = GradientBoostingRegressor()

voting_regressor = VotingRegressor(
    estimators=[
        ('Linear regressor',linear_regressor_model),
        ('xgboost', xgboost_regressor),
        ('random_forrest', random_forest_regressor),
        ('gradient_boosting', gradient_boosting_regressor)
    ],
    weights=[1, 1, 1,1],
    n_jobs=-1
)

voting_regressor.fit(Xtrain,Ytrain)
y_pred = voting_regressor.predict(Xtest)

mae=mean_absolute_error(y_pred,Ytest)
print("Mean Absolute Error:", mae)

mse= mean_squared_error(y_pred,Ytest)
print("Mean Squared Error:", mse)

rmse4 = np.sqrt(mse)
model_rms_scores.append(rmse4)
print("Root Mean Squared Error:",rmse4)

"""**RANDOM FOREST REGRESSOR**"""

rf_regressor = RandomForestRegressor(n_estimators=300, max_depth=40, n_jobs= -1)
rf_regressor.fit(Xtrain, Ytrain)
y_pred = rf_regressor.predict(Xtest)

mae=mean_absolute_error(y_pred,Ytest)
print("Mean Absolute Error:", mae)

mse= mean_squared_error(y_pred,Ytest)
print("Mean Squared Error:", mse)

rmse5 = np.sqrt(mse)
model_rms_scores.append(rmse5)
print("Root Mean Squared Error:",rmse5)

"""**CHOOSING THE BEST MODEL**"""

model_names = ["Decision Tree", "XGBoost", "Gradient Boosting", "Voting", "Random Forest"]
best_model_index = model_rms_scores.index(min(model_rms_scores))
best_model_name = model_names[best_model_index]
best_rmse = model_rms_scores[best_model_index]

print(f"The best model among the others is '{best_model_name}' with RMSE of {best_rmse:.2f}")

"""**CROSS VALIDATION  FOR MOST ACCURATE MODEL**"""

##Since Random Forest gave us the best RMSE, we will finetune that and use crossvalidation on it with GridSearchCV

# GridSearchCV combines hyperparameter tuning and cross-validation in a single process.
# It systematically searches through hyperparameter combinations, training and evaluating the model on multiple subsets of the data to find the best configuration.
# This ensures that the model is optimized for generalization to unseen data while fine-tuning its hyperparameters.
# Define the hyperparameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],  # Adjust the values as needed
    'max_depth': [10, 20, 30]  # Adjust the values as needed
}

# Create the Random Forest model
rf_model = RandomForestRegressor()



# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, n_jobs=-1)

# Fit the grid search to find the best hyperparameters
grid_search.fit(X, Y)

# Get the best hyperparameters
best_params = grid_search.best_params_

#Train the final model with the best hyperparameters
best_model = RandomForestRegressor(**best_params)
best_model.fit(X, Y)

"""# **AFTER CROSS VALIDATION THE BEST MODEL AND ITS HYPER PARAMETERS**"""

# Make predictions with the best model
Y_pred = best_model.predict(Xtest)

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(Ytest, Y_pred))

print("RMSE on the testÂ set:", rmse)

mae = mean_absolute_error(Ytest, Y_pred)

print("Mean Absolute Error (MAE) on the test set:", mae)

"""### **Use the data from another season(players_22) which was not used during the training to test how good is the model. [5]**

**FIFA 22**
"""

testdf = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AI/Midsem/players_22.csv')

testdf.info()

numeric_columns22 = testdf.select_dtypes(include=['float64','int64'])
missing_values22 = numeric_columns22.isnull().sum()
missing_values22

#replacing missing values with mean of its columns
testdf[numeric_columns22.columns] = testdf[numeric_columns22.columns].fillna(numeric_columns22.mean())
testdf

objects_columns22 = testdf.select_dtypes(include=['object'])

# dropping all unwanted categorica variables
testdf = testdf.select_dtypes(exclude=['object'])
testdf

feature_subsetx

"""TESTING WITH FIFA 22"""

subsetx22 = [
    'movement_reactions',
    'skill_dribbling',
    'passing',
    'potential',
    'dribbling',
    'attacking_short_passing',
    'physic',
    'skill_long_passing',
    'movement_agility',
    'skill_moves',
    'shooting',
    'skill_ball_control',
    'mentality_vision',
    'weight_kg',
    'attacking_crossing'
]
feature_subset22 =testdf[subsetx22]
feature_subset22

sc = StandardScaler()
scaled = sc.fit_transform(feature_subset22)
feature_subset22 = pd.DataFrame(scaled,columns = feature_subset22.columns)
feature_subset22

X_test = feature_subset22
X_test

##testing our model with fifa 22 dataset
prediction = best_model.predict(X_test)

true_rating = testdf['overall'].values
rmse = mean_squared_error(true_rating, prediction, squared = False)
print("RMSE on the testÂ set:", rmse)

# pickling the model
import pickle
pickle_out = open("bestmodel.pkl", "wb")
pickle.dump(best_model, pickle_out)
pickle_out.close()

